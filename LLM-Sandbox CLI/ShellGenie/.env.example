# ShellGenie Environment Variables
# Copy this file to .env and fill in your values

# Model Configuration
SHELLGENIE_MODEL=llama3.2
SHELLGENIE_BACKEND=ollama
OLLAMA_HOST=http://localhost:11434

# Security
SHELLGENIE_SECURITY=moderate
SHELLGENIE_AUTO_EXECUTE=false

# Logging
SHELLGENIE_LOG_LEVEL=INFO
SHELLGENIE_LOG_FILE=~/.shellgenie/shellgenie.log

# GPU Configuration (for llama.cpp)
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Optional: API Keys (if using cloud LLMs)
# OPENAI_API_KEY=your_key_here
# ANTHROPIC_API_KEY=your_key_here
