# GRYPHGEN LLM Image
# Version: 2.0
# Base: GRYPHGEN Base with PyTorch 2.5+
# Target: NVIDIA RTX 4080 16GB

FROM gryphgen-base:latest

# Switch to root for installations
USER root

# Copy GPU requirements
COPY configs/requirements-gpu.txt /tmp/requirements-gpu.txt

# Install PyTorch with CUDA 12.6
RUN pip install --no-cache-dir torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu126

# Install LLM packages
RUN pip install --no-cache-dir -r /tmp/requirements-gpu.txt && \
    rm /tmp/requirements-gpu.txt

# Install additional tools
RUN pip install --no-cache-dir \
    notebook \
    jupyterlab \
    tensorboard \
    wandb

# PyTorch optimizations for RTX 4080
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
ENV TORCH_CUDNN_V8_API_ENABLED=1

# Create model cache directories
RUN mkdir -p \
    /models/gryphgen/llama \
    /models/gryphgen/mistral \
    /models/gryphgen/codellama \
    /models/gryphgen/embeddings \
    /data/gryphgen/vector_db \
    /data/gryphgen/cache \
    && chown -R gryphgen:gryphgen /models/gryphgen /data/gryphgen

# Switch back to non-root user
USER gryphgen

# Set Hugging Face cache directory
ENV HF_HOME=/models/gryphgen/cache
ENV TRANSFORMERS_CACHE=/models/gryphgen/cache
ENV HF_DATASETS_CACHE=/data/gryphgen/cache

# Expose ports
EXPOSE 8000 8888 8080 6006

# Health check with GPU verification
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available(), 'CUDA not available'" || exit 1

WORKDIR /opt/gryphgen

CMD ["/bin/bash"]
