# NVIDIA RTX 4080 16GB Optimized Configuration
# Configuration for running multiple LLM models with IPC

[gpu]
# GPU specifications
name = NVIDIA GeForce RTX 4080
vram_gb = 16
vram_mb = 16384
usable_percentage = 90
architecture = Ada Lovelace
cuda_cores = 9728
tensor_cores = 304

[model_limits]
# Recommended model sizes for different use cases
# Values in GB

# Single model with large context (8K+ tokens)
single_large_context = 14.0

# Single model with normal context (4K tokens)
single_normal_context = 15.0

# Dual model setup (both models loaded)
dual_model_each = 7.0

# Multi-model setup (3+ models)
multi_model_each = 4.5

[ipc]
# Inter-Process Communication settings
pipe_dir = /tmp
pipe_prefix = model
default_timeout = 30
enable_logging = true
log_dir = /var/log/gryphgen

[performance]
# Performance optimization settings
batch_size = 512
threads = 16
use_mmap = true
use_mlock = false
low_vram = false

# Context settings
context_length = 4096
predict_tokens = 512

[monitoring]
# Monitoring and metrics
enable_gpu_monitoring = true
monitoring_interval = 5
log_memory_usage = true
alert_vram_threshold = 95

[paths]
# Default paths for models and cache
model_dir = /mnt/models
cache_dir = /var/cache/gryphgen
temp_dir = /tmp/gryphgen

[recommended_models]
# Recommended GGUF quantization formats for RTX 4080
# Format: param_count = max_quantization

7b = Q8_0
13b = Q6_K
30b = Q4_K_M
70b = Q2_K

[experimental]
# Experimental features
flash_attention = true
grouped_query_attention = true
rope_scaling = false
